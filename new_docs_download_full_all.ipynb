{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikiparser_utils import WikiXMLDump, WikiPage\n",
    "import os\n",
    "import nltk\n",
    "import wikitextparser as wtp\n",
    "import json\n",
    "import numpy as np\n",
    "from duckduckgo_search import ddg\n",
    "from tqdm import tqdm\n",
    "from difflib import Differ \n",
    "from utils.difflibparser import DifflibParser, DiffCode\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "from collections import Counter\n",
    "import os\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import mwxml\n",
    "import wikitextparser as wtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_DIR = 'data/documents_new'\n",
    "PAGES_DIR = 'data/revision_new'\n",
    "\n",
    "if not os.path.exists(DOCS_DIR):\n",
    "    os.makedirs(DOCS_DIR)\n",
    "    \n",
    "if not os.path.exists(PAGES_DIR):\n",
    "    os.makedirs(PAGES_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_page(page_name):\n",
    "    return bool(re.search('[a-zA-Z]', page_name))\n",
    "\n",
    "def filter_comment(comment_text, user_name):\n",
    "    com_text = comment_text.strip()\n",
    "    if 'bot' in com_text or 'bot' in user_name:\n",
    "        return False\n",
    "    \n",
    "    if com_text[-2:] == '*/':\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CHANGE_SYMB_LEN = 5\n",
    "MAX_CHANGE_SYMB_LEN = 300\n",
    "MAX_PAGE_SYMB_LEN = 20000\n",
    "MAX_ABSTRACT_LEN = 800\n",
    "MIN_ABSTRACT_LEN = 10\n",
    "MIN_COMMENT_LEN = 5\n",
    "abstarct_tokenizer = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(l, r, arr):\n",
    "    l_ans, r_ans = -1, -1\n",
    "    for sent_idx, (l_arr, r_arr) in enumerate(zip(arr, arr[1:])):\n",
    "        if l_arr <= l < r_arr:\n",
    "            l_ans = sent_idx\n",
    "        if l_arr <= r < r_arr:\n",
    "            r_ans = sent_idx\n",
    "    if l_ans == -1:\n",
    "        l_ans = len(arr) - 1\n",
    "    if r_ans == -1:\n",
    "        r_ans = len(arr) - 1\n",
    "    return l_ans, r_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('=====', '==').replace('====', '==').replace('===', '==')\n",
    "    text = re.sub('\\[\\[File.*?]]', '', text, count=0, flags=0)\n",
    "    text = re.sub('\\[\\[Category:.*?]]', '', text, count=0, flags=0)\n",
    "    text = re.sub('\\[\\[category:.*?]]', '', text, count=0, flags=0)\n",
    "    text = wtp.remove_markup(text)\n",
    "    text = text.replace('\\t', '').replace('\\n\\n\\n', '\\n\\n').replace('\\n\\n\\n', '\\n\\n')\n",
    "    text = text.replace('\\n\\n*', ', ').replace('\\n\\n*', ', ')\n",
    "    return text\n",
    "    \n",
    "def clean_section_text(text):\n",
    "    text = re.sub('==.*?==+', '', text, count=0, flags=0)\n",
    "    return text.strip()\n",
    "\n",
    "def text2sentences(text, sent_tokenizer=nltk.sent_tokenize):\n",
    "    idxs_arr = []\n",
    "    sents = sent_tokenizer(text)\n",
    "    cur_str = text[:]\n",
    "    cur_skip = 0\n",
    "    idxs2sent = {}\n",
    "    for sent in sents:\n",
    "        match_idx = cur_str.find(sent)\n",
    "        start_idx = match_idx + cur_skip\n",
    "        idxs_arr.append(start_idx)\n",
    "        finish_idx = match_idx + cur_skip + len(sent) - 1\n",
    "        idxs2sent[(start_idx, finish_idx)] = sent\n",
    "        if finish_idx + 1 < len(cur_str):\n",
    "            cur_skip = finish_idx + 1\n",
    "            cur_str = cur_str[match_idx + len(sent):]\n",
    "    return idxs2sent, np.array(sents), idxs_arr\n",
    "\n",
    "def extract_important_sections(text):\n",
    "    parsed_text = wtp.parse(text)\n",
    "    section_titles, section_texts = [], []\n",
    "    for sec in parsed_text.sections:\n",
    "        if not sec.title:\n",
    "            #for par in sec.string.split('\\n\\n'):\n",
    "            section_titles.append(sec.title)\n",
    "            section_texts.append(clean_section_text(sec.string))\n",
    "            continue\n",
    "        if 'external links' in sec.title.lower():\n",
    "            continue\n",
    "        if 'references' in sec.title.lower():\n",
    "            continue\n",
    "        if 'notes' in sec.title.lower():\n",
    "            continue\n",
    "        if 'see also' in sec.title.lower():\n",
    "            continue\n",
    "        \n",
    "        #for par in sec.string.split('\\n\\n'):\n",
    "        section_titles.append(sec.title)\n",
    "        section_texts.append(clean_section_text(sec.string))\n",
    "    return section_titles, section_texts\n",
    "\n",
    "def get_diff_num(prev_sections_texts, new_sections_texts):\n",
    "    prev_set = set(enumerate(prev_sections_texts))\n",
    "    new_set = set(enumerate(new_sections_texts))\n",
    "    new_diff = new_set - prev_set\n",
    "    prev_diff = prev_set - new_set\n",
    "    if len(new_diff) > 0 and len(prev_diff) > 0:\n",
    "        print('\\n\\n#############################################')\n",
    "        print('WAS:\\n')\n",
    "        print(new_diff)\n",
    "        print('\\n-------------------------------------------\\nNOW:\\n')\n",
    "        print(prev_diff)\n",
    "        print('\\n')\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_diff_num2(prev_sections_texts, new_sections_texts):\n",
    "    differ_obj = Differ()\n",
    "    dif_result = list(DifflibParser(prev_sections_texts, new_sections_texts))\n",
    "    result = []\n",
    "    result_idxs = []\n",
    "    old_text, new_text, last_diff_id = [], [], -1000\n",
    "    for dif_id, dif_line in enumerate(dif_result):\n",
    "        if dif_line['code'] != DiffCode.SIMILAR:\n",
    "            if np.abs(dif_id - last_diff_id) > 0:\n",
    "                result.append(dif_line)\n",
    "                result_idxs.append(dif_id)\n",
    "                last_diff_id = dif_id\n",
    "    return result_idxs, result    \n",
    "\n",
    "def get_changes(diffs):\n",
    "    all_changes = []\n",
    "    all_changes_sents = []\n",
    "    for diff_id, diff_obj in enumerate(diffs):\n",
    "        if diff_obj['code'] == DiffCode.RIGHTONLY:\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) > MAX_ABSTRACT_LEN:\n",
    "                continue\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) < MIN_ABSTRACT_LEN:\n",
    "                continue\n",
    "            all_changes.append(([diff_obj['line']], 'r'))\n",
    "            _, sents, _ = text2sentences(diff_obj['line'])\n",
    "            all_changes_sents.append(sents)\n",
    "            \n",
    "        elif diff_obj['code'] == DiffCode.LEFTONLY:\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) > MAX_ABSTRACT_LEN:\n",
    "                continue\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) < MIN_ABSTRACT_LEN:\n",
    "                continue\n",
    "            all_changes.append(([diff_obj['line']], 'l'))\n",
    "            _, sents, _ = text2sentences(diff_obj['line'])\n",
    "            all_changes_sents.append(sents)\n",
    "            \n",
    "        elif diff_obj['code'] == DiffCode.CHANGED:\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) > MAX_ABSTRACT_LEN:\n",
    "                continue\n",
    "            if len(abstarct_tokenizer(diff_obj['newline'])) > MAX_ABSTRACT_LEN:\n",
    "                continue\n",
    "            idxs2sent, sents, idxs_arr = text2sentences(diff_obj['newline'])\n",
    "            all_changes_sents = []\n",
    "            r_change = diff_obj['rightchanges']\n",
    "            cur_ch = -10\n",
    "            prev_ch = -10\n",
    "            all_r_changes = []\n",
    "            changed_sents = []\n",
    "            for ch in r_change:\n",
    "                if prev_ch < 0:\n",
    "                    prev_ch = ch\n",
    "                    cur_ch = ch\n",
    "                if np.abs(ch - cur_ch) > 1:\n",
    "                    new_change = diff_obj['newline'][prev_ch:cur_ch+1]\n",
    "                    if new_change.strip() != '' and len(new_change.strip()) > MIN_CHANGE_SYMB_LEN:\n",
    "                        all_r_changes.append(new_change)\n",
    "                        sents_idxs_l, sents_idxs_r = find_nearest(prev_ch, cur_ch+1, idxs_arr)\n",
    "                        changed_sents += list(range(sents_idxs_l, sents_idxs_r+1))\n",
    "                    prev_ch = ch\n",
    "                cur_ch = ch\n",
    "            new_change = diff_obj['newline'][prev_ch:cur_ch+1]\n",
    "            if new_change.strip() != '' and len(new_change.strip()) > MIN_CHANGE_SYMB_LEN:\n",
    "                all_r_changes.append(new_change)\n",
    "                sents_idxs_l, sents_idxs_r = find_nearest(prev_ch, cur_ch+1, idxs_arr)\n",
    "                changed_sents += list(range(sents_idxs_l, sents_idxs_r+1))\n",
    "            all_changes.append((all_r_changes, 'c'))\n",
    "            changed_sents = sorted(list(set(changed_sents)))\n",
    "            all_changes_sents.append(sents[changed_sents])\n",
    "    return all_changes, all_changes_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-30 16:02:02.062170\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOC_COUNTER = 0\n",
    "W2DC = Counter()\n",
    "dump = mwxml.Dump.from_file(open('data/history6_last.xml', encoding=\"utf-8\"))\n",
    "pbar = tqdm(position=0, leave=True)\n",
    "for page in dump:\n",
    "    revisions = []\n",
    "    for rev in page:\n",
    "        revisions.append(rev)\n",
    "    last_rev = revisions[-1]\n",
    "    last_rev_text = ''\n",
    "    if last_rev.text:\n",
    "        last_rev_text = clean_text(last_rev.text).lower()\n",
    "    \n",
    "    tokens = list(set(nltk.word_tokenize(last_rev_text)))\n",
    "    W2DC.update(tokens)\n",
    "    DOC_COUNTER += 1\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2DC.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOC_COUNTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-30 16:02:02.066901\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "start_time = datetime.datetime.now()\n",
    "print(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.headless = True\n",
    "ua = UserAgent()\n",
    "userAgent = ua.random\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "driver_main = webdriver.Chrome(options=options)\n",
    "driver_main.set_page_load_timeout(180)\n",
    "\n",
    "# открываем сайт с таблицой об игроках с players_ranks[0] до players_ranks[1] \n",
    "driver_main.get(f'https://duckduckgo.com/')\n",
    "    \n",
    "# временами спим, чтобы имитировать поведение пользователя\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current url is:https://html.duckduckgo.com/html/\n"
     ]
    }
   ],
   "source": [
    "get_url = driver_main.current_url\n",
    "print(\"The current url is:\"+str(get_url))\n",
    "if 'error' in driver_main.page_source:\n",
    "    print(driver_main.page_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_docs_by_query(query, driver_main):\n",
    "    get_url = driver_main.current_url\n",
    "    version = 'lite' \n",
    "    if 'lite.duckduckgo' in get_url:\n",
    "        version = 'lite'\n",
    "    elif 'html.duckduckgo' in get_url:\n",
    "        version = 'html'\n",
    "    else:\n",
    "        version = 'norm'\n",
    "    \n",
    "    if version == 'norm':\n",
    "        search_input_pattern = 'js-search-input' \n",
    "        submit_pattern = 'search__button' \n",
    "        result_snippet_pattern = 'result__snippet' \n",
    "        next_button = \"//input[@value='Next']\" \n",
    "    elif version == 'html':\n",
    "        search_input_pattern = 'search__input' \n",
    "        submit_pattern = 'search__button' \n",
    "        result_snippet_pattern = 'result__snippet' \n",
    "        next_button = \"//input[@value='Next']\" \n",
    "    else:\n",
    "        search_input_pattern = 'query'\n",
    "        submit_pattern = 'submit'\n",
    "        result_snippet_pattern = 'result-snippet' \n",
    "        next_button = \"//input[@value='Next Page >']\"\n",
    "    \n",
    "    search_form = driver_main.find_element(By.CLASS_NAME, search_input_pattern)\n",
    "    search_form.send_keys(query)\n",
    "    driver_main.find_element(By.CLASS_NAME, submit_pattern).click()\n",
    "    \n",
    "    snipets_texts = set()\n",
    "    for i in range(2):\n",
    "        snips = driver_main.find_elements(By.CLASS_NAME, result_snippet_pattern)\n",
    "        snips = snips if snips else []\n",
    "        for snip in snips:\n",
    "            snipets_texts.add(snip.text)\n",
    "    \n",
    "        try:\n",
    "            driver_main.find_element(By.XPATH, next_button).click()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        except:\n",
    "            print('BAD NEXT TRY', query)\n",
    "        \n",
    "    \n",
    "    search_form = driver_main.find_element(By.CLASS_NAME, search_input_pattern)\n",
    "    search_form.clear()\n",
    "    return list(snipets_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_doc_score(doc_text):\n",
    "    doc_toks = nltk.word_tokenize(doc_text.lower())\n",
    "    toks_counter = Counter(doc_toks)\n",
    "    score = 0.0\n",
    "    for tok, tok_count in toks_counter.items():\n",
    "        if W2DC[tok] > 0:\n",
    "            score += tok_count * np.log(DOC_COUNTER / W2DC[tok])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = find_docs_by_query('Henry Blogg Legacy', driver_main)\n",
    "driver_main.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In March 1946 Henry Blogg had just turned 70 - ten years over the retiring age for lifeboatmen. However he asked to serve one more year and in view of his unrivalled record of service - over 53 years the Cromer lifeboat had saved 873 lives - his request was granted. Henry George Blogg died in 1954 at the age of 78.', 'Henry Kissinger is one of the worst people to ever be a force for good. He manipulated colleagues and nations. He faked the beginning of a nuclear war in order to advance some perverse personal ...', 'According to Paige, Henry was always more carefree than his sisters. Legacy In high school, Henry Jr. was liked by everyone in school, despite not being involved in any social groups or clubs. He got average grades and dated girls without wanting to get serious. Despite their differences, he was also good friends with his cousin Payton.']\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    search_scored_result = [(st, count_doc_score(st)) for st in search_result]\n",
    "    sorted_search_scored_result = sorted(search_scored_result, key=lambda x: -x[1])\n",
    "    print(sorted_search_scored_result[:3])\n",
    "else:\n",
    "    print(search_result[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18938it [11:34, 27.26it/s]\n",
      "192178it [2:19:59, 42.64it/s] "
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "dump = mwxml.Dump.from_file(open('data/history6_last.xml', encoding=\"utf-8\"))\n",
    "pbar = tqdm(position=0, leave=True)\n",
    "\n",
    "for page in dump:\n",
    "    if not filter_page(page.title):\n",
    "        continue\n",
    "    revisions = []\n",
    "    for rev in page:\n",
    "        revisions.append(rev)\n",
    "    \n",
    "    if len(revisions) < 2: \n",
    "        continue\n",
    "    good_revisions = []\n",
    "    \n",
    "    last_added = len(revisions)\n",
    "    for cur_rev_id in range(len(revisions) - 1, 1, -1):\n",
    "        if cur_rev_id >= last_added:\n",
    "            continue\n",
    "        cur_rev = revisions[cur_rev_id]\n",
    "        if cur_rev.text:\n",
    "            cur_rev_text = cur_rev.text # clean_text(cur_rev.text)\n",
    "        else:\n",
    "            cur_rev_text = ''\n",
    "        \n",
    "        for new_rev_id in range(cur_rev_id, 0, -1):\n",
    "            new_rev = revisions[new_rev_id]\n",
    "            \n",
    "            if new_rev.text:\n",
    "                new_rev_text = new_rev.text # clean_text(new_rev.text)\n",
    "            else:\n",
    "                new_rev_text = ''\n",
    "            \n",
    "            if cur_rev_text == new_rev_text:\n",
    "                last_added = new_rev_id\n",
    "        \n",
    "        add_rev = revisions[last_added]\n",
    "        user = ''\n",
    "        if add_rev.user:\n",
    "            if add_rev.user.text:\n",
    "                user = add_rev.user.text.lower()\n",
    "        revision_dict = {\n",
    "            'text': cur_rev_text,\n",
    "            'comment': add_rev.comment,\n",
    "            'id': add_rev.id,\n",
    "            'page_name': page.title,\n",
    "            'user_name': user\n",
    "        }\n",
    "        good_revisions.append(revision_dict)\n",
    "    good_revisions = good_revisions[::-1]\n",
    "              \n",
    "    for prev_rev, new_rev in zip(good_revisions[:], good_revisions[1:]):\n",
    "        comment = new_rev['comment']\n",
    "        if comment and len(comment.strip()) > MIN_COMMENT_LEN:\n",
    "            if filter_comment(comment, new_rev['user_name']):\n",
    "                if np.abs(len(new_rev['text']) - len(prev_rev['text'])) > MIN_CHANGE_SYMB_LEN:\n",
    "                    if np.abs(len(new_rev['text']) - len(prev_rev['text'])) < MAX_CHANGE_SYMB_LEN:\n",
    "                        if np.abs(len(new_rev['text'])) < MAX_PAGE_SYMB_LEN:\n",
    "                            prev_section_titles, prev_section_texts = extract_important_sections(clean_text(prev_rev['text']))\n",
    "                            new_section_titles, new_section_texts = extract_important_sections(clean_text(new_rev['text']))\n",
    "\n",
    "                            r_idx, r = get_diff_num2(prev_section_texts, new_section_texts)\n",
    "                            if len(r) == 1 and 'newline' in r[0]:\n",
    "                                # print(1)\n",
    "                                section_name = ''\n",
    "                                try:\n",
    "                                    section_name_t = new_section_titles[r_idx[0]]\n",
    "                                    if section_name_t:\n",
    "                                        section_name = section_name_t\n",
    "                                except:\n",
    "                                    pass\n",
    "                                ts = new_rev['page_name'] + ' ' + section_name\n",
    "                                \n",
    "                                all_changes_r, all_changes_sents_r = get_changes(r)\n",
    "                                if len(all_changes_sents_r) > 0 and len(all_changes_sents_r[0]) > 0:\n",
    "                                    ts = new_rev['page_name'] + ' ' + section_name                                    \n",
    "                                    \n",
    "                                    final_page_path = f\"{PAGES_DIR}/{counter}.json\"\n",
    "                                    final_docs_path = f\"{DOCS_DIR}/{counter}.txt\"\n",
    "                                    if os.path.exists(final_page_path) and os.path.exists(final_docs_path):\n",
    "                                        counter += 1\n",
    "                                        continue\n",
    "                                    \n",
    "                                    downloaded_docs = []\n",
    "                                    search_queries_list = []\n",
    "                                    for ch_text_idx, ch_text in enumerate(all_changes_r[0][0]):\n",
    "                                            fq = ts.strip() + ' ' + ch_text\n",
    "                                            # print(f'Final search query {ch_text_idx}:\\t', fq)\n",
    "                                            search_queries_list.append(fq)\n",
    "                                            search_result = ddg(fq)\n",
    "                                            if search_result is not None:\n",
    "                                                for search_result_obj in search_result:\n",
    "                                                    downloaded_docs.append(search_result_obj['body'])\n",
    "                                    \n",
    "                                    json_obj = {\n",
    "                                        \"old_text\": r[0]['line'],\n",
    "                                        \"new_text\": r[0]['newline'],\n",
    "                                        \"title\": new_rev['page_name'],\n",
    "                                        \"comment\": comment,\n",
    "                                        \"section_name\": section_name,\n",
    "                                        \"search_queries\": search_queries_list,\n",
    "                                        \"change_texts\": all_changes_r\n",
    "                                    }\n",
    "                                    \n",
    "                                    final_page_path = f\"{PAGES_DIR}/{counter}.json\"\n",
    "                                    with open(final_page_path, 'w', encoding='utf-8') as f:\n",
    "                                        json.dump(json_obj, f)\n",
    "                                    # changed_text = [ctxt for ctxt in all_changes_r[0][0]]\n",
    "                                    # changed_text_full = ' '.join(changed_text)\n",
    "                                    \n",
    "                                    final_docs_path = f\"{DOCS_DIR}/{counter}.txt\"\n",
    "                                    with open(final_docs_path, 'w', encoding='utf-8') as f:\n",
    "                                        for doc_text_idx, doc_text in enumerate(downloaded_docs):\n",
    "                                            f.write(doc_text)\n",
    "                                            if doc_text_idx != len(downloaded_docs) - 1:\n",
    "                                                f.write(\"\\n\\nDOC_DELIMITER_TOKEN\\n\\n\")\n",
    "\n",
    "                                    counter += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_time = datetime.datetime.now()\n",
    "print(start_time)\n",
    "print(finish_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(start_time)\n",
    "print(finish_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34875"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "45 * 775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darkwood Dub_2935508\n"
     ]
    }
   ],
   "source": [
    "print(page_name)\n",
    "json.dump(page_docs_mapper_id2link, open(f'{DOCS_MAPPER_DIR}/{page_name}/id2link.json', \"w\"))\n",
    "json.dump(page_docs_mapper_link2id, open(f'{DOCS_MAPPER_DIR}/{page_name}/link2id.json', \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
