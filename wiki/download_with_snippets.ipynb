{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.commit_utils import *\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import subprocess\n",
    "import mwxml\n",
    "import numpy as np\n",
    "from duckduckgo_search import ddg\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_DIR = 'data/data_uploading/documents'\n",
    "PAGES_DIR = 'data/data_uploading/revision'\n",
    "DUMPS_DIR = 'data/data_uploading/dumps'\n",
    "\n",
    "if not os.path.exists(DOCS_DIR):\n",
    "    os.makedirs(DOCS_DIR)\n",
    "    \n",
    "if not os.path.exists(PAGES_DIR):\n",
    "    os.makedirs(PAGES_DIR)\n",
    "\n",
    "if not os.path.exists(DUMPS_DIR):\n",
    "    os.makedirs(DUMPS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_docs = os.listdir(DOCS_DIR)\n",
    "down_page = os.listdir(PAGES_DIR)\n",
    "len(down_docs), len(down_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMPS = [\n",
    "    # \"https://dumps.wikimedia.org/enwiki/20221001/enwiki-20221001-pages-meta-history2.xml-p60834p62692.7z\",\n",
    "    # \"https://dumps.wikimedia.org/enwiki/20221001/enwiki-20221001-pages-meta-history2.xml-p64379p66077.7z\",\n",
    "    # \"https://dumps.wikimedia.org/enwiki/20221001/enwiki-20221001-pages-meta-history2.xml-p66078p67792.7z\",\n",
    "    \"https://dumps.wikimedia.org/enwiki/20221001/enwiki-20221001-pages-meta-history2.xml-p67793p69852.7z\",\n",
    "    \"https://dumps.wikimedia.org/enwiki/20221001/enwiki-20221001-pages-meta-history2.xml-p69853p72172.7z\",\n",
    "    \"https://dumps.wikimedia.org/enwiki/20221001/enwiki-20221001-pages-meta-history25.xml-p60275287p60527634.7z\",\n",
    "    \"https://dumps.wikimedia.org/enwiki/20221001/enwiki-20221001-pages-meta-history25.xml-p60527635p60799673.7z\",\n",
    "    \"https://dumps.wikimedia.org/enwiki/20221001/enwiki-20221001-pages-meta-history25.xml-p60799674p61049885.7z\",\n",
    "    \"https://dumps.wikimedia.org/enwiki/20221001/enwiki-20221001-pages-meta-history25.xml-p61049886p61308529.7z\",\n",
    "    \"https://dumps.wikimedia.org/enwiki/20221001/enwiki-20221001-pages-meta-history25.xml-p61308530p61507683.7z\",\n",
    "    \"https://dumps.wikimedia.org/enwiki/20221001/enwiki-20221001-pages-meta-history25.xml-p61507684p61774102.7z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-14 14:48:52.275307\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "start_time = datetime.datetime.now()\n",
    "print(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link2names(link):\n",
    "    dump_name = link.split('-')[-1].split('.')[0]\n",
    "    dump_7z_name = link.split('20221001/')[1]\n",
    "    dump_unzip_name = dump_7z_name.split('.7z')[0]\n",
    "    return dump_name, dump_7z_name, dump_unzip_name\n",
    "\n",
    "def download_dump(dump_link):\n",
    "    download_command = f'wget {dump_link}'\n",
    "    download_result = subprocess.call(download_command, shell=True) \n",
    "    return download_result\n",
    "\n",
    "def unzip_dump(dump_7z_name):\n",
    "    unzip_command = f\"7za x {dump_7z_name}\"\n",
    "    unzip_result = subprocess.call(unzip_command, shell=True)\n",
    "    return unzip_result\n",
    "\n",
    "def delete_7z_dump(dump_7z_name):\n",
    "    delete_7z_command = f'rm -f {dump_7z_name}'\n",
    "    delete_7z_result = subprocess.call(delete_7z_command, shell=True)\n",
    "    return delete_7z_result\n",
    "\n",
    "def filter_duplicate8vandal_revisions(page):\n",
    "    num_revisions, revisions = 0, []\n",
    "    \n",
    "    for rev in page:\n",
    "        revisions.append(rev)\n",
    "            \n",
    "    if len(revisions) < 2: \n",
    "        return [], 0\n",
    "    \n",
    "    good_revisions = []\n",
    "    last_added = len(revisions)\n",
    "    for cur_rev_id in range(len(revisions) - 1, 0, -1):\n",
    "        if cur_rev_id >= last_added:\n",
    "            continue\n",
    "        cur_rev = revisions[cur_rev_id]\n",
    "        if cur_rev.text:\n",
    "            cur_rev_text = cur_rev.text # clean_text(cur_rev.text)\n",
    "        else:\n",
    "            cur_rev_text = ''\n",
    "\n",
    "        for new_rev_id in range(cur_rev_id, -1, -1):\n",
    "            new_rev = revisions[new_rev_id]\n",
    "\n",
    "            if new_rev.text:\n",
    "                new_rev_text = new_rev.text # clean_text(new_rev.text)\n",
    "            else:\n",
    "                new_rev_text = ''\n",
    "\n",
    "            if cur_rev_text == new_rev_text:\n",
    "                last_added = new_rev_id\n",
    "\n",
    "        add_rev = revisions[last_added]\n",
    "        user = ''\n",
    "        if add_rev.user:\n",
    "            if add_rev.user.text:\n",
    "                user = add_rev.user.text.lower()\n",
    "        revision_dict = {\n",
    "            'text': cur_rev_text,\n",
    "            'comment': add_rev.comment,\n",
    "            'id': add_rev.id,\n",
    "            'page_name': page.title,\n",
    "            'user_name': user\n",
    "        }\n",
    "        good_revisions.append(revision_dict)\n",
    "        num_revisions += 1\n",
    "    good_revisions = good_revisions[::-1]\n",
    "    return good_revisions, num_revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112553it [6:58:48,  2.95s/it]"
     ]
    }
   ],
   "source": [
    "dump_link = 'https://dumps.wikimedia.org/enwiki/20221001/enwiki-20221001-pages-meta-history2.xml-p67793p69852.7z'\n",
    "dump_name, dump_7z_name, dump_unzip_name = link2names(dump_link)\n",
    "dump = mwxml.Dump.from_file(open(f'{dump_unzip_name}', encoding=\"utf-8\"))\n",
    "counter, total_pair_rev_counter, total_revisions_counter = 0, 0, 0\n",
    "if not os.path.exists(f\"{DOCS_DIR}/{dump_name}\"):\n",
    "    os.makedirs(f\"{DOCS_DIR}/{dump_name}\")\n",
    "if not os.path.exists(f\"{PAGES_DIR}/{dump_name}\"):\n",
    "    os.makedirs(f\"{PAGES_DIR}/{dump_name}\")\n",
    "\n",
    "pbar = tqdm(position=0, leave=True)\n",
    "EMPTY_DOCS = 0\n",
    "for page in dump:\n",
    "    if not filter_page(page.title):\n",
    "        continue\n",
    "\n",
    "    good_revisions, num_revisions = filter_duplicate8vandal_revisions(page)\n",
    "    total_revisions_counter += num_revisions\n",
    "\n",
    "    for prev_rev, new_rev in zip(good_revisions[:], good_revisions[1:]):\n",
    "        total_pair_rev_counter += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "        comment = new_rev['comment']\n",
    "        if not comment or len(comment.strip()) < MIN_COMMENT_LEN:\n",
    "            continue\n",
    "        if not filter_comment(comment, new_rev['user_name']):\n",
    "            continue\n",
    "\n",
    "        if np.abs(len(new_rev['text']) - len(prev_rev['text'])) < MIN_CHANGE_SYMB_LEN:\n",
    "            continue\n",
    "\n",
    "        if np.abs(len(new_rev['text']) - len(prev_rev['text'])) > MAX_CHANGE_SYMB_LEN:\n",
    "            continue\n",
    "\n",
    "        if np.abs(len(new_rev['text'])) > MAX_PAGE_SYMB_LEN:\n",
    "            continue\n",
    "\n",
    "        prev_section_titles, prev_section_texts = extract_important_sections(clean_text(prev_rev['text']))\n",
    "        new_section_titles, new_section_texts = extract_important_sections(clean_text(new_rev['text']))\n",
    "\n",
    "        r_idx, r = get_diff_num(prev_section_texts, new_section_texts)\n",
    "        if len(r) != 1 or 'newline' not in r[0]:\n",
    "            continue\n",
    "\n",
    "        section_name = ''\n",
    "        try:\n",
    "            section_name_t = new_section_titles[r_idx[0]]\n",
    "            if section_name_t:\n",
    "                section_name = section_name_t\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # print()\n",
    "        ts = new_rev['page_name'] + ' ' + section_name\n",
    "        all_changes_r, all_changes_sents_r = get_changes(r)\n",
    "        if len(all_changes_sents_r) == 0 or len(all_changes_sents_r[0]) == 0:\n",
    "            continue\n",
    "\n",
    "        final_page_path = f\"{PAGES_DIR}/{dump_name}/{counter}.json\"\n",
    "        final_docs_path = f\"{DOCS_DIR}/{dump_name}/{counter}.txt\"\n",
    "        if os.path.exists(final_page_path) and os.path.exists(final_docs_path):\n",
    "            counter += 1\n",
    "            continue\n",
    "\n",
    "        downloaded_docs = []\n",
    "        search_queries_list = []\n",
    "        q2docs_num = []\n",
    "        \n",
    "        for ch_text_idx, ch_text in enumerate(all_changes_r[0][0][:10]):\n",
    "            fq = ts.strip() + ' ' + ch_text\n",
    "            fq = fq[:300]\n",
    "            # print(f'Final search query {ch_text_idx}:\\t', fq)\n",
    "            search_queries_list.append(fq)\n",
    "            search_result = ddg(fq)\n",
    "            counter_found_docs = 0\n",
    "            if search_result is not None:\n",
    "                for search_result_obj in search_result:\n",
    "                    downloaded_docs.append(search_result_obj['body'])\n",
    "                    counter_found_docs += 1\n",
    "            q2docs_num.append(counter_found_docs)\n",
    "                     \n",
    "        for ch_text_idx, ch_text in enumerate(all_changes_sents_r[0][:5]):\n",
    "            fq = ts.strip() + ' ' + ch_text\n",
    "            fq = fq[:300]\n",
    "            # print(f'Final search query {ch_text_idx}:\\t', fq)\n",
    "            search_queries_list.append(fq)\n",
    "            search_result = ddg(fq)\n",
    "            counter_found_docs = 0\n",
    "            if search_result is not None:\n",
    "                for search_result_obj in search_result:\n",
    "                    downloaded_docs.append(search_result_obj['body'])\n",
    "                    counter_found_docs += 1\n",
    "            q2docs_num.append(counter_found_docs)\n",
    "\n",
    "        json_obj = {\n",
    "            \"old_text\": r[0]['line'],\n",
    "            \"new_text\": r[0]['newline'],\n",
    "            \"title\": new_rev['page_name'],\n",
    "            \"comment\": comment,\n",
    "            \"section_name\": section_name,\n",
    "            \"search_queries\": search_queries_list,\n",
    "            \"counter_found_docs\": q2docs_num,\n",
    "            \"change_texts\": all_changes_r\n",
    "        }\n",
    "\n",
    "        with open(final_page_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_obj, f)\n",
    "\n",
    "        with open(final_docs_path, 'w', encoding='utf-8') as f:\n",
    "            for doc_text_idx, doc_text in enumerate(downloaded_docs):\n",
    "                f.write(doc_text)\n",
    "                if doc_text_idx != len(downloaded_docs) - 1:\n",
    "                    f.write(\"\\n\\nDOC_DELIMITER_TOKEN\\n\\n\")\n",
    "\n",
    "        # counter += 1\n",
    "        # if len(downloaded_docs) < 2:\n",
    "        #    EMPTY_DOCS += 1\n",
    "        #    if EMPTY_DOCS > 40:\n",
    "        #        time.sleep(120)\n",
    "        # else:\n",
    "        #    EMPTY_DOCS = 0\n",
    "\n",
    "delete_dump_command = f'rm -f {dump_unzip_name}'\n",
    "delete_dump_result = subprocess.call(delete_dump_command, shell=True) \n",
    "if delete_dump_result != 0:\n",
    "    print(f\"ERROR_DELETE_DUMP_{dump_unzip_name}\")\n",
    "print(\"SUCCESS\", dump_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dump_link in DUMPS:\n",
    "    dump_name, dump_7z_name, dump_unzip_name = link2names(dump_link)\n",
    "    \n",
    "    final_commits = f\"{PAGES_DIR}/{dump_name}\"\n",
    "    final_docs = f\"{DOCS_DIR}/{dump_name}\"\n",
    "    if os.path.exists(final_commits) and os.path.exists(final_docs):\n",
    "        print(\"EXISTS\", dump_link, '\\t\\t', dump_name)\n",
    "        continue\n",
    "\n",
    "    download_result = download_dump(dump_link)\n",
    "    if download_result != 0:\n",
    "        print(f\"ERROR_DOWNLOAD_{dump_link}\")\n",
    "        continue\n",
    "              \n",
    "    unzip_result = unzip_dump(dump_7z_name)\n",
    "    if unzip_result != 0:\n",
    "        print(f\"ERROR_UNZIP_{dump_7z_name}\")\n",
    "        continue\n",
    "\n",
    "    delete_7z_result = delete_7z_dump(dump_7z_name)\n",
    "    if delete_7z_result != 0:\n",
    "        print(f\"ERROR_DELETE_{dump_7z_name}\")\n",
    "        continue\n",
    "    \n",
    "    dump = mwxml.Dump.from_file(open(f'{dump_unzip_name}', encoding=\"utf-8\"))\n",
    "    counter, total_pair_rev_counter, total_revisions_counter = 0, 0, 0\n",
    "    if not os.path.exists(f\"{DOCS_DIR}/{dump_name}\"):\n",
    "        os.makedirs(f\"{DOCS_DIR}/{dump_name}\")\n",
    "    if not os.path.exists(f\"{PAGES_DIR}/{dump_name}\"):\n",
    "        os.makedirs(f\"{PAGES_DIR}/{dump_name}\")\n",
    "    \n",
    "    pbar = tqdm(position=0, leave=True)\n",
    "    EMPTY_DOCS = 0\n",
    "    for page in dump:\n",
    "        if not filter_page(page.title):\n",
    "            continue\n",
    "\n",
    "        good_revisions, num_revisions = filter_duplicate8vandal_revisions(page)\n",
    "        total_revisions_counter += num_revisions\n",
    "\n",
    "        for prev_rev, new_rev in zip(good_revisions[:], good_revisions[1:]):\n",
    "            total_pair_rev_counter += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            comment = new_rev['comment']\n",
    "            if not comment or len(comment.strip()) < MIN_COMMENT_LEN:\n",
    "                continue\n",
    "            if not filter_comment(comment, new_rev['user_name']):\n",
    "                continue\n",
    "                    \n",
    "            if np.abs(len(new_rev['text']) - len(prev_rev['text'])) < MIN_CHANGE_SYMB_LEN:\n",
    "                continue\n",
    "            \n",
    "            if np.abs(len(new_rev['text']) - len(prev_rev['text'])) > MAX_CHANGE_SYMB_LEN:\n",
    "                continue\n",
    "                            \n",
    "            if np.abs(len(new_rev['text'])) > MAX_PAGE_SYMB_LEN:\n",
    "                continue\n",
    "                                \n",
    "            prev_section_titles, prev_section_texts = extract_important_sections(clean_text(prev_rev['text']))\n",
    "            new_section_titles, new_section_texts = extract_important_sections(clean_text(new_rev['text']))\n",
    "\n",
    "            r_idx, r = get_diff_num(prev_section_texts, new_section_texts)\n",
    "            if len(r) != 1 or 'newline' not in r[0]:\n",
    "                continue\n",
    "            \n",
    "            section_name = ''\n",
    "            try:\n",
    "                section_name_t = new_section_titles[r_idx[0]]\n",
    "                if section_name_t:\n",
    "                    section_name = section_name_t\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            ts = new_rev['page_name'] + ' ' + section_name\n",
    "            all_changes_r, all_changes_sents_r = get_changes(r)\n",
    "            if len(all_changes_sents_r) == 0 or len(all_changes_sents_r[0]) == 0:\n",
    "                continue\n",
    "                                               \n",
    "            final_page_path = f\"{PAGES_DIR}/{dump_name}/{counter}.json\"\n",
    "            final_docs_path = f\"{DOCS_DIR}/{dump_name}/{counter}.txt\"\n",
    "            if os.path.exists(final_page_path) and os.path.exists(final_docs_path):\n",
    "                counter += 1\n",
    "                continue\n",
    "\n",
    "            downloaded_docs = []\n",
    "            search_queries_list = []\n",
    "            q2docs_num = []\n",
    "            '''\n",
    "            for ch_text_idx, ch_text in enumerate(all_changes_r[0][0]):\n",
    "                fq = ts.strip() + ' ' + ch_text\n",
    "                # print(f'Final search query {ch_text_idx}:\\t', fq)\n",
    "                search_queries_list.append(fq)\n",
    "                search_result = ddg(fq)\n",
    "                counter_found_docs = 0\n",
    "                if search_result is not None:\n",
    "                    for search_result_obj in search_result:\n",
    "                        downloaded_docs.append(search_result_obj['body'])\n",
    "                        counter_found_docs += 1\n",
    "                q2docs_num.append(counter_found_docs)\n",
    "            '''               \n",
    "            for ch_text_idx, ch_text in enumerate(all_changes_sents_r[0]):\n",
    "                ch_text = ' '.join(ch_text)\n",
    "                fq = ts.strip() + ' ' + ch_text\n",
    "                # print(f'Final search query {ch_text_idx}:\\t', fq)\n",
    "                search_queries_list.append(fq)\n",
    "                search_result = ddg(fq)\n",
    "                counter_found_docs = 0\n",
    "                if search_result is not None:\n",
    "                    for search_result_obj in search_result:\n",
    "                        downloaded_docs.append(search_result_obj['body'])\n",
    "                        counter_found_docs += 1\n",
    "                q2docs_num.append(counter_found_docs)\n",
    "\n",
    "            json_obj = {\n",
    "                \"old_text\": r[0]['line'],\n",
    "                \"new_text\": r[0]['newline'],\n",
    "                \"title\": new_rev['page_name'],\n",
    "                \"comment\": comment,\n",
    "                \"section_name\": section_name,\n",
    "                \"search_queries\": search_queries_list,\n",
    "                \"counter_found_docs\": q2docs_num,\n",
    "                \"change_texts\": all_changes_r\n",
    "            }\n",
    "\n",
    "            with open(final_page_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(json_obj, f)\n",
    "\n",
    "            with open(final_docs_path, 'w', encoding='utf-8') as f:\n",
    "                for doc_text_idx, doc_text in enumerate(downloaded_docs):\n",
    "                    f.write(doc_text)\n",
    "                    if doc_text_idx != len(downloaded_docs) - 1:\n",
    "                        f.write(\"\\n\\nDOC_DELIMITER_TOKEN\\n\\n\")\n",
    "\n",
    "            # counter += 1\n",
    "            # if len(downloaded_docs) < 2:\n",
    "            #    EMPTY_DOCS += 1\n",
    "            #    if EMPTY_DOCS > 40:\n",
    "            #        time.sleep(120)\n",
    "            # else:\n",
    "            #    EMPTY_DOCS = 0\n",
    "        \n",
    "    delete_dump_command = f'rm -f {dump_unzip_name}'\n",
    "    delete_dump_result = subprocess.call(delete_dump_command, shell=True) \n",
    "    if delete_dump_result != 0:\n",
    "        print(f\"ERROR_DELETE_DUMP_{dump_unzip_name}\")\n",
    "    print(\"SUCCESS\", dump_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_name, dump_link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
