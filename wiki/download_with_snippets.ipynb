{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.wikiparser_utils import WikiXMLDump, WikiPage\n",
    "import os\n",
    "import nltk\n",
    "import wikitextparser as wtp\n",
    "import json\n",
    "import numpy as np\n",
    "from duckduckgo_search import ddg\n",
    "from tqdm import tqdm\n",
    "from difflib import Differ \n",
    "from utils.difflibparser import DifflibParser, DiffCode\n",
    "# from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "import os\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import mwxml\n",
    "import wikitextparser as wtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_DIR = 'downloaded_data/documents_new_val'\n",
    "PAGES_DIR = 'downloaded_data/revision_new_val'\n",
    "\n",
    "if not os.path.exists(DOCS_DIR):\n",
    "    os.makedirs(DOCS_DIR)\n",
    "    \n",
    "if not os.path.exists(PAGES_DIR):\n",
    "    os.makedirs(PAGES_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_docs = os.listdir(DOCS_DIR)\n",
    "down_page = os.listdir(PAGES_DIR)\n",
    "len(down_docs), len(down_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_page(page_name):\n",
    "    return bool(re.search('[a-zA-Z]', page_name))\n",
    "\n",
    "def filter_comment(comment_text, user_name):\n",
    "    com_text = comment_text.strip()\n",
    "    if 'bot' in com_text or 'bot' in user_name:\n",
    "        return False\n",
    "    \n",
    "    if com_text[-2:] == '*/':\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CHANGE_SYMB_LEN = 5\n",
    "MAX_CHANGE_SYMB_LEN = 300\n",
    "MAX_PAGE_SYMB_LEN = 20000\n",
    "MAX_ABSTRACT_LEN = 800\n",
    "MIN_ABSTRACT_LEN = 10\n",
    "MIN_COMMENT_LEN = 5\n",
    "abstarct_tokenizer = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(l, r, arr):\n",
    "    l_ans, r_ans = -1, -1\n",
    "    for sent_idx, (l_arr, r_arr) in enumerate(zip(arr, arr[1:])):\n",
    "        if l_arr <= l < r_arr:\n",
    "            l_ans = sent_idx\n",
    "        if l_arr <= r < r_arr:\n",
    "            r_ans = sent_idx\n",
    "    if l_ans == -1:\n",
    "        l_ans = len(arr) - 1\n",
    "    if r_ans == -1:\n",
    "        r_ans = len(arr) - 1\n",
    "    return l_ans, r_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('=====', '==').replace('====', '==').replace('===', '==')\n",
    "    text = re.sub('\\[\\[File.*?]]', '', text, count=0, flags=0)\n",
    "    text = re.sub('\\[\\[Category:.*?]]', '', text, count=0, flags=0)\n",
    "    text = re.sub('\\[\\[category:.*?]]', '', text, count=0, flags=0)\n",
    "    text = wtp.remove_markup(text)\n",
    "    text = text.replace('\\t', '').replace('\\n\\n\\n', '\\n\\n').replace('\\n\\n\\n', '\\n\\n')\n",
    "    text = text.replace('\\n\\n*', ', ').replace('\\n\\n*', ', ')\n",
    "    return text\n",
    "    \n",
    "def clean_section_text(text):\n",
    "    text = re.sub('==.*?==+', '', text, count=0, flags=0)\n",
    "    return text.strip()\n",
    "\n",
    "def text2sentences(text, sent_tokenizer=nltk.sent_tokenize):\n",
    "    idxs_arr = []\n",
    "    sents = sent_tokenizer(text)\n",
    "    cur_str = text[:]\n",
    "    cur_skip = 0\n",
    "    idxs2sent = {}\n",
    "    for sent in sents:\n",
    "        match_idx = cur_str.find(sent)\n",
    "        start_idx = match_idx + cur_skip\n",
    "        idxs_arr.append(start_idx)\n",
    "        finish_idx = match_idx + cur_skip + len(sent) - 1\n",
    "        idxs2sent[(start_idx, finish_idx)] = sent\n",
    "        if finish_idx + 1 < len(cur_str):\n",
    "            cur_skip = finish_idx + 1\n",
    "            cur_str = cur_str[match_idx + len(sent):]\n",
    "    return idxs2sent, np.array(sents), idxs_arr\n",
    "\n",
    "def extract_important_sections(text):\n",
    "    parsed_text = wtp.parse(text)\n",
    "    section_titles, section_texts = [], []\n",
    "    for sec in parsed_text.sections:\n",
    "        if not sec.title:\n",
    "            #for par in sec.string.split('\\n\\n'):\n",
    "            section_titles.append(sec.title)\n",
    "            section_texts.append(clean_section_text(sec.string))\n",
    "            continue\n",
    "        if 'external links' in sec.title.lower():\n",
    "            continue\n",
    "        if 'references' in sec.title.lower():\n",
    "            continue\n",
    "        if 'notes' in sec.title.lower():\n",
    "            continue\n",
    "        if 'see also' in sec.title.lower():\n",
    "            continue\n",
    "        \n",
    "        #for par in sec.string.split('\\n\\n'):\n",
    "        section_titles.append(sec.title)\n",
    "        section_texts.append(clean_section_text(sec.string))\n",
    "    return section_titles, section_texts\n",
    "\n",
    "def get_diff_num(prev_sections_texts, new_sections_texts):\n",
    "    prev_set = set(enumerate(prev_sections_texts))\n",
    "    new_set = set(enumerate(new_sections_texts))\n",
    "    new_diff = new_set - prev_set\n",
    "    prev_diff = prev_set - new_set\n",
    "    if len(new_diff) > 0 and len(prev_diff) > 0:\n",
    "        print('\\n\\n#############################################')\n",
    "        print('WAS:\\n')\n",
    "        print(new_diff)\n",
    "        print('\\n-------------------------------------------\\nNOW:\\n')\n",
    "        print(prev_diff)\n",
    "        print('\\n')\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_diff_num2(prev_sections_texts, new_sections_texts):\n",
    "    differ_obj = Differ()\n",
    "    dif_result = list(DifflibParser(prev_sections_texts, new_sections_texts))\n",
    "    result = []\n",
    "    result_idxs = []\n",
    "    old_text, new_text, last_diff_id = [], [], -1000\n",
    "    for dif_id, dif_line in enumerate(dif_result):\n",
    "        if dif_line['code'] != DiffCode.SIMILAR:\n",
    "            if np.abs(dif_id - last_diff_id) > 0:\n",
    "                result.append(dif_line)\n",
    "                result_idxs.append(dif_id)\n",
    "                last_diff_id = dif_id\n",
    "    return result_idxs, result    \n",
    "\n",
    "def get_changes(diffs):\n",
    "    all_changes = []\n",
    "    all_changes_sents = []\n",
    "    for diff_id, diff_obj in enumerate(diffs):\n",
    "        if diff_obj['code'] == DiffCode.RIGHTONLY:\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) > MAX_ABSTRACT_LEN:\n",
    "                continue\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) < MIN_ABSTRACT_LEN:\n",
    "                continue\n",
    "            all_changes.append(([diff_obj['line']], 'r'))\n",
    "            _, sents, _ = text2sentences(diff_obj['line'])\n",
    "            all_changes_sents.append(sents)\n",
    "            \n",
    "        elif diff_obj['code'] == DiffCode.LEFTONLY:\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) > MAX_ABSTRACT_LEN:\n",
    "                continue\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) < MIN_ABSTRACT_LEN:\n",
    "                continue\n",
    "            all_changes.append(([diff_obj['line']], 'l'))\n",
    "            _, sents, _ = text2sentences(diff_obj['line'])\n",
    "            all_changes_sents.append(sents)\n",
    "            \n",
    "        elif diff_obj['code'] == DiffCode.CHANGED:\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) > MAX_ABSTRACT_LEN:\n",
    "                continue\n",
    "            if len(abstarct_tokenizer(diff_obj['newline'])) > MAX_ABSTRACT_LEN:\n",
    "                continue\n",
    "            idxs2sent, sents, idxs_arr = text2sentences(diff_obj['newline'])\n",
    "            all_changes_sents = []\n",
    "            r_change = diff_obj['rightchanges']\n",
    "            cur_ch = -10\n",
    "            prev_ch = -10\n",
    "            all_r_changes = []\n",
    "            changed_sents = []\n",
    "            for ch in r_change:\n",
    "                if prev_ch < 0:\n",
    "                    prev_ch = ch\n",
    "                    cur_ch = ch\n",
    "                if np.abs(ch - cur_ch) > 1:\n",
    "                    new_change = diff_obj['newline'][prev_ch:cur_ch+1]\n",
    "                    if new_change.strip() != '' and len(new_change.strip()) > MIN_CHANGE_SYMB_LEN:\n",
    "                        all_r_changes.append(new_change)\n",
    "                        sents_idxs_l, sents_idxs_r = find_nearest(prev_ch, cur_ch+1, idxs_arr)\n",
    "                        changed_sents += list(range(sents_idxs_l, sents_idxs_r+1))\n",
    "                    prev_ch = ch\n",
    "                cur_ch = ch\n",
    "            new_change = diff_obj['newline'][prev_ch:cur_ch+1]\n",
    "            if new_change.strip() != '' and len(new_change.strip()) > MIN_CHANGE_SYMB_LEN:\n",
    "                all_r_changes.append(new_change)\n",
    "                sents_idxs_l, sents_idxs_r = find_nearest(prev_ch, cur_ch+1, idxs_arr)\n",
    "                changed_sents += list(range(sents_idxs_l, sents_idxs_r+1))\n",
    "            all_changes.append((all_r_changes, 'c'))\n",
    "            changed_sents = sorted(list(set(changed_sents)))\n",
    "            all_changes_sents.append(sents[changed_sents])\n",
    "    return all_changes, all_changes_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-12 19:18:43.697625\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOC_COUNTER = 0\n",
    "W2DC = Counter()\n",
    "dump = mwxml.Dump.from_file(open('data/history6_last.xml', encoding=\"utf-8\"))\n",
    "pbar = tqdm(position=0, leave=True)\n",
    "for page in dump:\n",
    "    revisions = []\n",
    "    for rev in page:\n",
    "        revisions.append(rev)\n",
    "    last_rev = revisions[-1]\n",
    "    last_rev_text = ''\n",
    "    if last_rev.text:\n",
    "        last_rev_text = clean_text(last_rev.text).lower()\n",
    "    \n",
    "    tokens = list(set(nltk.word_tokenize(last_rev_text)))\n",
    "    W2DC.update(tokens)\n",
    "    DOC_COUNTER += 1\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2DC.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOC_COUNTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-12 19:18:43.701608\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "start_time = datetime.datetime.now()\n",
    "print(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_doc_score(doc_text):\n",
    "    doc_toks = nltk.word_tokenize(doc_text.lower())\n",
    "    toks_counter = Counter(doc_toks)\n",
    "    score = 0.0\n",
    "    for tok, tok_count in toks_counter.items():\n",
    "        if W2DC[tok] > 0:\n",
    "            score += tok_count * np.log(DOC_COUNTER / W2DC[tok])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['val_dump.xml']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DUMPS = os.listdir('dump')\n",
    "DUMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7321it [03:56,  5.72it/s, downloaded_docs=149]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mwtypes/timestamp.py\u001b[0m in \u001b[0;36mfrom_string\u001b[0;34m(cls, string)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSHORT_MW_TIME_STRING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mwtypes/timestamp.py\u001b[0m in \u001b[0;36mstrptime\u001b[0;34m(cls, string, format)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \"\"\"\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_time_struct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime_time\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    570\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstruct_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_STRUCT_TM_ITEMS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    358\u001b[0m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0;32m--> 359\u001b[0;31m                          (data_string, format))\n\u001b[0m\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data '2018-04-22T19:18:11Z' does not match format '%Y%m%d%H%M%S'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6afee87cfcbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mrevisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mrevisions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mwxml/iteration/page.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrevision\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__revisions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mrevision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mwxml/iteration/page.py\u001b[0m in \u001b[0;36mload_revisions\u001b[0;34m(cls, first_revision, element)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"revision\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mRevision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 raise MalformedXML(\"Expected to see <revision>.  \" +\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mwxml/iteration/revision.py\u001b[0m in \u001b[0;36mfrom_element\u001b[0;34m(cls, element)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmwtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"contributor\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0muser_deleted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'deleted'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mwtypes/timestamp.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, time_thing)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_unix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_thing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_thing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mwtypes/timestamp.py\u001b[0m in \u001b[0;36mfrom_string\u001b[0;34m(cls, string)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLONG_MW_TIME_STRING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mwtypes/timestamp.py\u001b[0m in \u001b[0;36mstrptime\u001b[0;34m(cls, string, format)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \"\"\"\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_time_struct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime_time\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"Return a time struct based on the input string and the\n\u001b[1;32m    570\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstruct_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_STRUCT_TM_ITEMS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_cache_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mlocale_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TimeRE_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocale_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         if (_getlang() != locale_time.lang or\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtzname\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlocale_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtzname\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             time.daylight != locale_time.daylight):\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_getlang\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_getlang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Figure out what the current language is set to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLC_TIME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLocaleTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/locale.py\u001b[0m in \u001b[0;36mgetlocale\u001b[0;34m(category)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLC_ALL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m';'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocalename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category LC_ALL is not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parse_localename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msetlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/locale.py\u001b[0m in \u001b[0;36m_parse_localename\u001b[0;34m(localename)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m     \"\"\"\n\u001b[0;32m--> 481\u001b[0;31m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'@'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;31m# Deal with locale modifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "total_counter = 0\n",
    "\n",
    "for dump_name in DUMPS:\n",
    "    dump = mwxml.Dump.from_file(open(f'dump/{dump_name}', encoding=\"utf-8\"))\n",
    "    pbar = tqdm(position=0, leave=True)\n",
    "\n",
    "    for page in dump:\n",
    "        if not filter_page(page.title):\n",
    "            continue\n",
    "        revisions = []\n",
    "        for rev in page:\n",
    "            revisions.append(rev)\n",
    "\n",
    "        if len(revisions) < 2: \n",
    "            continue\n",
    "        good_revisions = []\n",
    "\n",
    "        last_added = len(revisions)\n",
    "        for cur_rev_id in range(len(revisions) - 1, 1, -1):\n",
    "            if cur_rev_id >= last_added:\n",
    "                continue\n",
    "            cur_rev = revisions[cur_rev_id]\n",
    "            if cur_rev.text:\n",
    "                cur_rev_text = cur_rev.text # clean_text(cur_rev.text)\n",
    "            else:\n",
    "                cur_rev_text = ''\n",
    "\n",
    "            for new_rev_id in range(cur_rev_id, 0, -1):\n",
    "                new_rev = revisions[new_rev_id]\n",
    "\n",
    "                if new_rev.text:\n",
    "                    new_rev_text = new_rev.text # clean_text(new_rev.text)\n",
    "                else:\n",
    "                    new_rev_text = ''\n",
    "\n",
    "                if cur_rev_text == new_rev_text:\n",
    "                    last_added = new_rev_id\n",
    "\n",
    "            add_rev = revisions[last_added]\n",
    "            user = ''\n",
    "            if add_rev.user:\n",
    "                if add_rev.user.text:\n",
    "                    user = add_rev.user.text.lower()\n",
    "            revision_dict = {\n",
    "                'text': cur_rev_text,\n",
    "                'comment': add_rev.comment,\n",
    "                'id': add_rev.id,\n",
    "                'page_name': page.title,\n",
    "                'user_name': user\n",
    "            }\n",
    "            good_revisions.append(revision_dict)\n",
    "        good_revisions = good_revisions[::-1]\n",
    "\n",
    "        for prev_rev, new_rev in zip(good_revisions[:], good_revisions[1:]):\n",
    "            total_counter += 1\n",
    "            comment = new_rev['comment']\n",
    "            if comment and len(comment.strip()) > MIN_COMMENT_LEN:\n",
    "                if filter_comment(comment, new_rev['user_name']):\n",
    "                    if np.abs(len(new_rev['text']) - len(prev_rev['text'])) > MIN_CHANGE_SYMB_LEN:\n",
    "                        if np.abs(len(new_rev['text']) - len(prev_rev['text'])) < MAX_CHANGE_SYMB_LEN:\n",
    "                            if np.abs(len(new_rev['text'])) < MAX_PAGE_SYMB_LEN:\n",
    "                                prev_section_titles, prev_section_texts = extract_important_sections(clean_text(prev_rev['text']))\n",
    "                                new_section_titles, new_section_texts = extract_important_sections(clean_text(new_rev['text']))\n",
    "\n",
    "                                r_idx, r = get_diff_num2(prev_section_texts, new_section_texts)\n",
    "                                if len(r) == 1 and 'newline' in r[0]:\n",
    "                                    # print(1)\n",
    "                                    section_name = ''\n",
    "                                    try:\n",
    "                                        section_name_t = new_section_titles[r_idx[0]]\n",
    "                                        if section_name_t:\n",
    "                                            section_name = section_name_t\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                    ts = new_rev['page_name'] + ' ' + section_name\n",
    "\n",
    "                                    all_changes_r, all_changes_sents_r = get_changes(r)\n",
    "                                    if len(all_changes_sents_r) > 0 and len(all_changes_sents_r[0]) > 0:\n",
    "                                        ts = new_rev['page_name'] + ' ' + section_name                                    \n",
    "\n",
    "                                        final_page_path = f\"{PAGES_DIR}/{counter}.json\"\n",
    "                                        final_docs_path = f\"{DOCS_DIR}/{counter}.txt\"\n",
    "                                        if os.path.exists(final_page_path) and os.path.exists(final_docs_path):\n",
    "                                            counter += 1\n",
    "                                            continue\n",
    "\n",
    "                                        downloaded_docs = []\n",
    "                                        search_queries_list = []\n",
    "                                        q2docs_num = []\n",
    "                                        for ch_text_idx, ch_text in enumerate(all_changes_r[0][0]):\n",
    "                                                fq = ts.strip() + ' ' + ch_text\n",
    "                                                # print(f'Final search query {ch_text_idx}:\\t', fq)\n",
    "                                                search_queries_list.append(fq)\n",
    "                                                search_result = ddg(fq)\n",
    "                                                counter_found_docs = 0\n",
    "                                                if search_result is not None:\n",
    "                                                    for search_result_obj in search_result:\n",
    "                                                        downloaded_docs.append(search_result_obj['body'])\n",
    "                                                        counter_found_docs += 1\n",
    "                                                q2docs_num.append(counter_found_docs)\n",
    "\n",
    "                                        json_obj = {\n",
    "                                            \"old_text\": r[0]['line'],\n",
    "                                            \"new_text\": r[0]['newline'],\n",
    "                                            \"title\": new_rev['page_name'],\n",
    "                                            \"comment\": comment,\n",
    "                                            \"section_name\": section_name,\n",
    "                                            \"search_queries\": search_queries_list,\n",
    "                                            \"counter_found_docs\": q2docs_num,\n",
    "                                            \"change_texts\": all_changes_r\n",
    "                                        }\n",
    "\n",
    "                                        final_page_path = f\"{PAGES_DIR}/{counter}.json\"\n",
    "                                        with open(final_page_path, 'w', encoding='utf-8') as f:\n",
    "                                            json.dump(json_obj, f)\n",
    "                                        # changed_text = [ctxt for ctxt in all_changes_r[0][0]]\n",
    "                                        # changed_text_full = ' '.join(changed_text)\n",
    "\n",
    "                                        final_docs_path = f\"{DOCS_DIR}/{counter}.txt\"\n",
    "                                        with open(final_docs_path, 'w', encoding='utf-8') as f:\n",
    "                                            for doc_text_idx, doc_text in enumerate(downloaded_docs):\n",
    "                                                f.write(doc_text)\n",
    "                                                if doc_text_idx != len(downloaded_docs) - 1:\n",
    "                                                    f.write(\"\\n\\nDOC_DELIMITER_TOKEN\\n\\n\")\n",
    "\n",
    "                                        counter += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(downloaded_docs=counter)\n",
    "            with open('total_counter', 'w') as f:\n",
    "                f.write(str(total_counter))\n",
    "            with open('counter', 'w') as f:\n",
    "                f.write(str(counter))\n",
    "            \n",
    "            if counter > 120:\n",
    "                break\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 7321)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7321it [04:10,  5.72it/s, downloaded_docs=149]"
     ]
    }
   ],
   "source": [
    "counter, total_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_time = datetime.datetime.now()\n",
    "print(start_time)\n",
    "print(finish_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(start_time)\n",
    "print(finish_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "45 * 775"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
